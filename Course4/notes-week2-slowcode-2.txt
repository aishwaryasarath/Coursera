A profiler is a tool that measures the resources that our code is using, giving us a better understanding of what's going on. In particular, they help us see how the memory is allocated and how the time spent
gprof and c-profiler

using right data structures

Lists are sequences of elements. We can add, remove, or modify the elements in them. We can iterate through the whole list to operate on each of the elements. Different programming languages call them differently.

 It's fast to access the element in a specific position in the list, but finding an element in an unknown position requires going through the whole list. This can be super slow if the list is long. Dictionary store key value pairs. We add data by associating a value to a key. Then, we retrieve a value by looking up a specific key. They are called HashMap in Java, Unordered Map in C++, Hash in Ruby, and Map in Go. The map part in those names comes from how we're creating a mapping between a key and a value. The Hash part comes from the fact that to make the structure efficient, a hashing function is used internally to decide how the elements will be stored. The main characteristic of this structure is that it's super-fast for looking up keys.


So as a rule of thumb, if you need to access elements by position or will always iterate through all the elements, use a list to store them. This could be a list of all computers in the network, of all employees in the company, or of all products currently on sale for example. On the flip side, if we need to look up the elements using a key, we'll use a dictionary. This could be the data associated to a user which we'd look up using their username, the IP associated to a computer using the host name, or the data associated to a product using the internal product code.

Loops are what make our computers do things repeatedly. They are an extremely useful tool and let us avoid repetitive work, but we need to use them with caution. In particular, we need to think about what actions we're going to do inside the loop, and when possible, avoid doing expensive actions. If you do an expensive operation inside a loop, you multiply the time it takes to do the expensive operation by the amount of times you repeat the loop. Say for example that you're writing a script to send an email to all the employees at your company asking them to verify that their emergency contact information is still valid. To send this out, you'll have a loop that sends one email per employee. In the body of the email, you'll include the current emergency contact data. The interesting part is how you access the data inside the loop. If the data is stored in a file, your script will need to parse the file to fetch it. If the script reads the whole file for every user, you'll be wasting a lot of time parsing the file over and over unnecessarily. Instead, you could parse the file outside of the loop, put the information into a dictionary, and then use the dictionary to retrieve the data inside the loop. Whenever you have a loop in your code, make sure to check what actions you're doing, and see if there are operations you can take out of the loop to do them just once. Instead of making one network call for each element, make one call before the loop. Instead of reading from disk for each element, read the whole thing before the loop. Even if the operations done inside the loop aren't especially expensive, if we're going through a list of a thousand elements and we only need five out of them, we're wasting time on elements we don't need. Make sure that the list of elements that you're iterating through is only as long as you really need it to be. Let's say you're running an internal website. As part of the information the site shows, it displays a list of the last five users that logged in. In the code, the program keeps a list of all the users that have logged in since it last started, and when the program needs to display the five latest users, it goes through the whole list and finds out which of those are the five most recent. This wastes a lot of time. If the service has been running for a while, it can take really long to go through the whole list. Instead, you could modify the service to store the user access info in log files that can be read if necessary and only keep the last five logins in memory. Whenever a new user logs in, the oldest entry in the list gets discarded and a new one gets added. That way, the script doesn't need to go through the whole list every time it needs to display the five most recent users. Another thing to remember about loops is to break out of the loop once you found what you were looking for. In Python, we do this using the keyword break. Breaking out of loops means that as soon as the data we're looking for is found, our script can continue. Of course if the data is at the end of the list, then we need to go through the loop anyway. But when the data is at the beginning of the list and not at the end, it makes sense to have our code break early to make the script faster. Say you're writing a script that checks if a given username is within the list of authorized entities, and if it is, it grants them access to a particular resource. You can use a for loop to iterate through the list of entities. When the username is found, you can break out of the loop and continue the rest of the script. One last thing to keep in mind is that the right solution for one problem might not be right for a different problem. Say your service has a total of 20 users. In that case, it's okay to go over this list whenever you want to check something. It's short enough that you don't need to do any special optimization. But if your service has over a thousand users, you'll want to avoid going through that list unless absolutely necessary. If the service has hundreds of thousands of users, going through that list isn't even a possibility. Up next, we'll talk about what we can do when the expensive operations are already outside of our loops


cd meeting_reminder_v2/
time ./send_reminders.py "2020-01-13|Eample|test1"
Real - actual time to execute the command ie wall clock time
user - time in user space
sys - time spent doing system-level operations

time ./send_reminders.py "2020-01-13|Eample|test1,test2,test3,test4,test5,
test6,test7,test8,test9"

pprofile3 -f callgrind -o profile.out ./send_reminders.py "2020-01-13|Eample|test1,test2,test3,test4,test5,
test6,test7,test8,test9"

kcachegrind profile.out

threading / asyncIO in python



We've called that a few times already, that reading information from disk or transferring it over the network is a slow operation. In typical scripts while this operation is going on, nothing else happens. The script is blocked, waiting for input or output while the CPU sits idle. One way we can make this better is to do operations in parallel. That way, while the computer is waiting for the slow IO, other work can take place. The tricky part is dividing up the tasks so that we get the same result in the end. There's actually a whole field of computer science called concurrency, dedicated to how we write programs that do operations in parallel. We won't go into a ton of details here but we'll give you a brief overview of what you can do. First, we need to understand what the operating system already does for us. Our OS handles the many processes that run on our computer. If a computer has more than one core, the operating system can decide which processes get executed on which core, and no matter the split between cores, all of these processes will be executing in parallel. Each of them has its own memory allocation and does its own IO calls. The OS will decide what fraction of CPU time each process gets and switch between them as needed. So a very easy way to run operations in parallel is just to split them across different processes, calling your script many times each with a different input set, and just let the operating system handle the concurrency. Let's say you want to collect statistics on the current load and memory usage for all the computers in your network. You can do this by writing a script that connects to each computer in a list and gets the stats. Each connection takes a while to complete, so the total run-time of the script would be the sum of the time taken by all those connections. Instead, you could split the list of computers into smaller groups and use the OS to call the script many times once for each group. That way, the connections to the different computers can be started in parallel, which minimizes the time but the CPU isn't doing anything. This is super easy to do and for many scripts, it'll be the right choice. Another easy thing to do, is to have a good balance of different workloads that you run on a computer. If you have a process that's using a lot of CPU while a different process is using a lot of network IO and another process is using a lot of disk IO, these can all run in parallel without interfering with each other. When using the OS to split the work and the processes, these processes don't share any memory, and sometimes we might need to have some shared data. In that case, we'd use threads. Threads let us run parallel tasks inside a process. This allows threats to share some of the memory with other threads in the same process. Since this isn't handled by the OS, we'll need to modify our code to create and handle the threats. For that, we'll need to look into how the programming language we're using implements threading. In Python, we can use the Threading or AsyncIO modules to do this. These modules let us specify which parts of the code we want to run in separate threads or as separate asynchronous events, and how we want the results of each to be combined in the end. We won't go into the details of how you can do that here, but we'll link to more information on this in the next reading. One thing to watch out for is that depending on the actual threading implementation for the language you're using, it might happen that all threads get executed in the same CPU processor. In that case, if you want to use more processors, you'll need to split the code into fully separate processes. If your script is mostly just waiting on input or output, also known as I/O bound, it might matter if it's executed on one processor or eight. But you might be doing this in parallel because you're using all of the available CPU time. In other words, your script is CPU bound. In this case, you'll definitely want to split your execution across processors. Now there's a point where adding more parallel processes means things become even slower, not faster. If we're trying to read a bunch of files from disk and do too many operations in parallel, the disk might end up spending more time going from one position to another then actually retrieving the data, or if we're doing a ton of operations that use a lot of CPU, the OS could spend more time switching between them than actually making progress in the calculations we're trying to do. So when doing operations in parallel, we need to find the right balance of simultaneous actions that let our computers stay busy without starving our system for resources. I recently felt the benefits of applying concurrency. I was working on migrating data that was stored in one format, and I needed to store it in a different format. There were a lot of gigabytes of data that needed migrating, so of course I wasn't going to do it manually. My first version of the script was taking an average of one hour per gigabyte migrated. This was much slower than I expected, so I decided to spend more time tweaking the code to make the migration go faster. I reorganized the logic to have a separate thread per file which decreased the total time to work through the files since it now wasn't a linear process, and then, to make it go even faster, I split the work onto different machines, each running a bunch of threads. After all this rearranging to use the resources I have, I brought it down to three minutes per gigabyte. Yeah, we may be learning a lot about slowness, but we sure are moving fast. Some of these concepts may feel a little complex and it's perfectly normal. Take your time, everyone learns slowness at their own pace. Up next, we'll discuss the different approaches we can take to handle systems as they become more complex.



memecached,
varnished
