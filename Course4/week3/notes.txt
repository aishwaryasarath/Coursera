In this video, we're going to talk about system crashes.
Start transcript at 13 seconds0:13
Got you. You should have seen the look on your face. But seriously, there are a ton of different reasons why applications crash. When we come across a program that terminates unexpectedly, we go through our usual cycle of gathering information about the crash, digging in until we find the root cause, and then applying the right fix. Say for example that a user asks for your help with a problem on their computer. When you ask for details, the user tells you that the internal billing application crashed while they were trying to generate an invoice for a customer. Now, this could be caused by lots of different things. So what you need to do is reduce the scope of the problem, and remember, you want to start with the actions that are easier and faster to check. As a first step, you tried looking at the logs to see if there's any error that may point to what's happening, but you only find an error saying application terminated and no useful information. So you check if the user can reproduce the problem by doing this same action on a different computer. You ask the user to try this out, and it turns out on a different machine that can generate the invoice just fine. So that means that the problem just has to do with the installation or configuration on that specific computer. Great news. You've already reduced the scope to something machine-specific. Another thing that you might want to check is if this happens reliably. Do all invoice generations fail? Is it confined to one specific product or customer? For this example, let's say that when you ask the user to try generating other invoices, it works just fine even for the same customer. Okay, you think maybe this problem was with a specific order for that specific customer on that specific computer. That's rather suspicious, but not so fast. The user tells you that after creating all the invoices for the day, they tried to generate a report, and the application crashed again. But then it worked the next time. You double-check with other users and find out the application isn't crashing when they use it. So what does this mean? The application seems to be crashing randomly but only on that computer. To further reduce the scope, you'll want to know if it's just that application or the whole system. To check this out, you can try moving away the local configuration for the program and using the default configuration instead, or maybe even reinstalling the application. You might also ask the user if they've seen crashes on any other application. For this example, let's say that reinstalling the application and running it with the default configuration still leads to random crashes. I'm impressed to remember, the user tells you that their web browser also crashed last week when they were using the internal webmail. At this point, the information points to a problem in the overall system, either the hardware or the OS installation. If you have a spare computer available, it might make sense to give one to the user at this point so that they can go back to work while you try to figure out the root cause of the problem. What can you do to further reduce the scope? By now, there's a high likelihood if the problem being hardware related. So one thing you could do is try taking the hard drive out of the computer and putting it into a different computer. This works best when you already have a spare case that you know works well so that you can use it for tests like these. That way you can quickly check if it's a problem with the data and the drive or the rest of the computer. Let's say that after putting the hard drive in the other computer, the applications run without unexpected crashes. This means that some hardware component is at fault. The next step is to find out which one. Given the random crashes, one thing to check would be the RAM. Memory chips deteriorate over time. When they do, the computer might write data to some part of the memory and then get a totally different value when trying to read it back. To check the health of our RAM, we can use the memtest 86 tool to look for errors. We run this tool on boot instead of the normal operating system so that it can access all of the available memory and verify if the data written to memory is the same when it tries to read it back. If the RAM is fine, you can check if the computer's overheating by looking at the sensor data provided by the OS. If that's not the case, check if there's a problem with external devices like a graphics card or sound card. You can do this by disconnecting or replacing the devices present in the computer and checking if the crashes still occur. So what can you do if when putting the hard drive in a separate computer, you still get the strange caches? This means the problem is in the drive itself or the OS installation. As with RAM, our hard drives age. At some point, the data that the computer reads stops matching what was originally stored. Each OS ships its own battery of hard drive checking tools, and you should familiarize yourself with ones in the OS you're working with. You'll want to look at the output of the tools that check the disk for bad sectors, and you'll also want to use these smart tools which can help detect errors and even try to anticipate problems before they affect the computer's performance. What can you do with the hard-drive turns out to be fine? You'd need to look into the possible OS issues, but before doing that, ask yourself, is it worth it? Looking to what's wrong with the installation can take a lot of valuable time. If the installation is easy to replicate, then just reinstalling the OS might be faster and simpler than looking into why it broke. Alright, so that's a glimpse of how you can try to diagnose a system that's unstable and behaving in weird ways. But often, you'll be dealing with a specific application that's misbehaving. In this case, it's almost certainly above in the application's code that's not taking into account a situation that, though unexpected, can sometimes occur. Up next, we'll check out what you can do when that happens.

0:00
[MUSIC] When an application crashes and we don't know why we'll want to look for logs that might relate to the failure. To look at logs on Linux will open the system log files and VAR log or the user log files and dot accession errors file. On Mac OS we generally use the console app to look at logs and the event Viewer on Windows. So what kind of data should you look for in these logs most logs have a date and time for each line locked knowing when the application crashed you can look for a log line around that time. And try to find an error message related to the application that crashed. Sometimes the errors will be self-explanatory like permission denied no such file or directory connection refused. Sometimes it will be a cryptic message and you have no idea what it means. Ever we have an error message no matter how weird it seems we can search for it online to try to figure out its meaning. If we're lucky, we might find the official documentation of what that error means and what we can do about it. But even if that's not available, will usually come across posts by others who have tackled a similar error and this additional information can help us understand what's going on. If there are no errors or the errors aren't useful we can try to find out more info by enabling sling debug logging. Many applications generate a lot more output when debugging logging is enabled. We might need to enable it from a setting in the applications configuration file or a command line parameter to pass when running the application manually. By enabling this extra logging information, we can get a better idea of what's actually causing the problem. And what do we need to do if there are no logs or error messages at all. In that case we need to use tools that let us see what going on inside the program. We call that a few are ready. On Linux we use S Trace to see what system calls a programs doing. The equivalent tool is called de trois on Mac OS process monitor is a Windows tool that can also take a peek inside what's going on inside a process on Windows?
Start transcript at 2 minutes 19 seconds2:19
By tracing which system calls a program is doing we can see what files and directories it's trying open what network connections it's trying to make and what information it's trying to read or write. This can give us a better idea of what caused the actual problem. We could find that the problem is caused by a resource not being present that the program expects to be present. Like we saw with the missing directory example in the earlier module or we could find that the program tries to interact with the graphics interface and there isn't any because it's a service running on a server. Or the program tries to open a file but the user running the software doesn't have the necessary permissions. If the application used to work fine and recently started crashing. It's useful to look into what changed in between. The first thing is to check if the issue is caused by a new version of the application itself. Maybe there's a bug in the new version that causes the crash or maybe the way that we're using the application is no longer supported. But that's not the only possible change that could trigger crashes. It could also be that a library or service used by our application changed and they no longer work well together or it could be that there was a configuration change in the overall environment. Like if the user isn't in a specific group anymore or if the files that the application used are in a different location. When trying to figure out what changed logs can also be a useful source of information. In the system log we can check which programs and libraries were recently updated checking for configuration changes might be harder depending on how you manage that configuration. If the settings are managed through a configuration management system and the values are stored in a Version Control System. Then you might be able to look at the history of changes and figure out which one triggered the failure. We call that a few times already how important it is to have a reproduction case for a problem that we're trying to solve. When we're trying to debug an application that crashes finding a reproduction case can help us both understand what's causing the crash and figure out what we can do to fix it. So it's valuable to spend some time figuring out the state that triggers the crash. This includes the overall system environment the specific application configuration the inputs to the application the outputs generated by the application the resources that uses and the services it communicates with. When trying create the reproduction case it might be useful to start from a clean slate and slowly put the pieces in place until the crash triggers. This might include trying out the application with the default configuration instead of the local one or on a freshly installed computer instead of the computer where it's crashing. And remember we want to make the reproduction case as small as possible this lets us better understand the problem and also quickly check if its present or not when we attempt to fix it. And even if we end up unable to fix the issue having a small and simple reproduction case is extremely helpful in reporting a bug to the program's developers. So to sum this up to find the root cause of a crashing application will want to look at all available logs figure out what changed trace the system or library calls the program makes and create the smallest possible reproduction case.
Start transcript at 5 minutes 47 seconds5:47
After doing all of this, we should have some idea of what the root cause of the issue is and maybe even how to fix it.
Start transcript at 5 minutes 55 seconds5:55
The strategy for fixing problems will depend on whether we can fix the code or not. In our next video, we'll check out what you can do when you can't fix the program and need to work around the issue. And in later videos, we'll deep dive into strategies for fixing faulty code.

One of the great things about working in IT is that we can tell the computer what to do and it will follow our orders. When dealing with unexpected behavior in the software written by other people though, we might not always be so lucky. It could be that we're dealing with proprietary software and the source code isn't available at all, or we might have access to the source code but it's written in a language that we don't understand and so we can't change it. No matter the reason, what can you do if you need to fix an application that crashes and you can't change the code. You'll need to figure out a way of working around the problem and avoiding the crash. The actual workaround will depend on what the issue is that you're trying to solve. Let's do a rundown of some of the available options. Say you figured out that the issue was caused by a specific data input that makes the application crash. The crashes only happen when the input isn't in the format the code expects. Some of your systems generate data in XML format which used to work fine with the previous version of the software but the new version now requires all data to be in a YAML format. In this case you can write a script that pre-processes the data and make sure that it's in the format that the program expects. Similarly if the problem is caused by an external service that the application uses and that's no longer compatible, we could write a service to act as a proxy and make sure that both sides see the requests and responses they expect. This type of compatibility layer is called a Wrapper. A Wrapper is a function or program that provides a compatibility layer between two functions or programs so they can work well together. Using Wrappers is a pretty common technique when the expected output and input formats don't match. So if you're faced with some sort of compatibility problem don't be afraid to write a Wrapper to work around it. Another possibility you might need to look at is if the overall system environment is it working well with the application. In this case, you might want to check what environment the applications developers recommend and then modify your systems to match that. This could be running the same version of the operating system using the same version of the dynamic libraries or interacting with the same back end services. Say the application was developed and tested on Windows 7, if you run into problems while trying to run it under Windows 10, you might want to use Windows 7 instead or if the application was developed and tested for Ubuntu and you're having trouble running it under Fedora, you might want to try running it on Ubuntu instead, and what can you do if you can't make the environment match? This could happen, for example, if there's another application that requires a different version of the same library or you can't change a certain configuration setting because it's required to access a different service. In this case, you might want to consider running the application inside a virtual machine or maybe a container. These are two different things but we won't go into details of how they are different here. All you need to know right now is they both let you run the affected application in its own environment without interfering with the rest of the system. This is what we need if we want the environment to be different than the one other Applications are using on the same computer. Sometimes we can't find a way to stop an application from crashing but we can make sure that if it crashes it starts back again. To do this, we can deploy a watchdog. This is a process that checks whether a program is running and when it's not starts the program again. To implement this, we need to write a script that stays running in the background and periodically checks if the other program is running. Whenever the check fails the watchdog will trigger the program to restart. Doing this won't avoid the crash itself. But it will at least ensure that the service is available. This works well for services where availability matters more than running continuously and no matter how you work around the issue, remember to always report the bug to the application developers. As we called out, if you have a good reproduction case for your issue, it makes it easier for the developers to figure out what's wrong and how to fix it. So when you report a bug make sure you include as much information as possible, share good reproduction case and answer the questions that we mentioned earlier on. What were you trying to do? What were the steps you followed? What did you expect to happen? What was the actual outcome? Up next, we'll see how to apply these skills to troubleshoot an application that's crashing.


Accessging invalid memory
In our earlier videos, we looked into a bunch of different things that can make software crash and what we can do about them when we can't fix the code. If we're able to make the application behave correctly though, we'll have a lot more options for dealing with the crash. Of course to apply these fixes, we'll need to understand why the crash is even happening. One common reason a program crashes is it's trying to access invalid memory. To understand what this means, let's quickly explain how using the memory works on modern operating systems. Each process running on our computer asks the operating system for a chunk of memory. This is the memory used to store values and do operations on them during the program's execution. The OS keeps a mapping table of which process is assigned which portion of the memory. Processes aren't allowed to read or write outside of the portions of memory they were assigned. So accessing invalid memory means that the process tried to access a portion of the system's memory that wasn't assigned to it. Now, how does this even happen? During normal working conditions, applications will request a portion of the memory and then use the space at the OS assigned to them. But programming errors might lead to a process trying to read or write to a memory address outside of the valid range. When this happens, the OS will raise an error like segmentation fault or general protection fault. What kind of programming error is this? It typically happens with low-level languages like C or C++ where the programmer needs to take care of requesting the memory that the program is going to use and then giving that memory back once it's not needed anymore. In these languages, the variables that store memory addresses are called pointers. They're just like any other variable and code that can be modified as needed. So if a pointer is set to a value outside of the valid memory range for that process, it will point to invalid memory. If the code then tries to access the memory the pointer points to, the application will crash. Common programming errors that lead to segmentation faults or segfaults include forgetting to initialize a variable, trying to access a list element outside of the valid range, trying to use a portion of memory after having given it back, and trying to write more data than the requested portion of memory can hold. So what can you do if you have a program that's said vaulting? The best way to understand what's going on is to attach a debugger to the faulty program. This way when the program crashes, you'll get information about the function where the fault happened. You'll know the parameters that the function received and find out the address that was invalid. That might already be enough to understand the problem. Maybe a certain variable is being initialized to late or the code is trying to read too many items on a list. If that's not enough, the debugger can give you a lot more detail on what the application is doing and why the memories invalid. For this to be possible, we'll need our program to be compiled with debugging symbols. This means that on top of the information that the computer uses to execute the program, the executable binary needs to include extra information needed for debugging, like the names of the variables and functions being used. These symbols are usually stripped away from the binaries that we run to make them smaller. So we'll need to either recompile the binary to include the symbols, or download the debugging symbols from the provider of the software if they're available. Linux distributions like Debian or Ubuntu ships separate packages with the debugging symbols for all the packages in the distribution. So to debug and application that's segfaulting, we download the debugging symbols for that application. Attach a debugger to it, and see where the fault occurs. When doing this, we might find that the crash happens inside a call to a library function. This is separate from the application itself, so we need to install the debugging symbols for that library. We might need to repeat this cycle a few times before we can identify the portion of the code that's buggy. Microsoft compilers can also generate debugging symbols in a separate PDB file. Some Windows software providers let users download the PDP files that correspond to their binaries to let them properly debug failures. One of the trickiest things about this invalid memory business is that we're usually dealing with undefined behavior. This means that the code is doing something that's not valid in the programming language. The actual outcome will depend on the compiler used, how the operating system assigns memory to processes, and even the version of the libraries in use. A program that runs fine on a computer running Windows trigger a segfault on a computer running Linux and vice versa. When trying to understand problems related to handling invalid memory, valgrind can help us a lot. Valgrind is a very powerful tool that can tell us if the code is doing any invalid operations no matter if it crashes are not. Valgrind lets us know if the code is accessing variables before initializing them. If the code is failing to free some of the memory requested, if the pointers are pointing to an invalid memory address, and a ton more things. Valgrind is available on Linux and Mac OS, and Dr. Memory is a similar tool that can be used on both Windows and Linux. So all of that said, what do we do when we finally discover the cause of the segfaults? You'll want to either change the code yourself or get the developers to fix the problem in the next version. This might sound scary if you've never programmed in the language used by the application. But when you know what's wrong with the code, it's usually not that hard to figure out how to fix it. If a variable is initialized too late, fixing the problem can be as easy as moving the initialization to the right part of the code, or if a loop is accessing an item outside of the length of the list, you might solve the issue by checking that there aren't more iterations than needed. Throughout this program, we've been teaching you these concepts so you can apply them to any piece of code no matter which language the program is using. So don't be afraid to put this into practice. You've got the skills for it. If the program is part of an open source project, you might find that someone else has already done the work, and so you can apply a patch available online. If there's no patch and you can't say you're the bug out yourself, you can always get in touch with the developers and ask a fake and fix the issue and create the necessary patch. In high-level languages like Python, the interpreter will almost certainly catch these problems itself. It will then throw an exception instead of letting the invalid memory access reach the operating system. But still those exceptions can be pretty annoying. We'll talk about those in our next video.

Unhandled Errors and Exceptions
In our last video, we talked a lot about what happens when a program tries to access invalid memory. Correctly handling memory is a hard problem, and that's why there's a bunch of different programming languages like Python, Java, or Ruby that do it for us. But that doesn't mean programs written in these languages can't trigger weird problems. In these languages, when a program comes across an unexpected condition that isn't correctly handled in the code, it will trigger errors or exceptions. In Python, for example, we could get an index error if we tried to access an element after the end of a list. We might get a type error or an attribute error if we try to take an action on a variable that wasn't properly initialized or division by zero error if we tried to well, divide by zero. When the code generates one of these errors without handling it properly, the program will finish unexpectedly. In general, unhandled errors happen because the codes making wrong assumptions maybe the program's trying to access a resource that's not present or the code assumes that the user will enter a value but the user entered and empty string instead. Or maybe the application is trying to convert a value from one format to another and the value doesn't match the initial expectations. When these failures happen, the interpreter that's running the program will print the type of error, the line that caused the failure, and the traceback. The traceback shows the lines of the different functions that were being executed when the problem happened. In lots of cases, the error message and traceback info already gives us enough to understand what's going on, and we can move on to solving the problem. But sadly, that's not always the case. The fact that a piece of code crashes on one function doesn't mean that the error is necessarily in that function. It's possible, for example, that the problem was caused by a function called earlier which set a variable to a bad value. So the function where the code crashes is just accessing that variable. So when the error message isn't enough, we'll need to debug the code to find out where things are going wrong. For that, we can use the debugging tools available for the application's language. For a Python, program we can use the BDB interactive debugger which lets us do all the typical debugging actions like executing lines of code one-by-one or looking at how the variables change values. When we're trying to understand what's up with a misbehaving function on top of using debuggers, it's common practice to add statements that print data related to the codes execution. Statements like these could show the contents of variables, the return values of functions or metadata like the length of a list or size of a file. This technique is called print f debugging. The name comes from the print f function used to print messages to the screen in the C programming language. But we can use this technique in all languages, no matter if we use print, puts, or echo to display the text on the screen. Let's take this one step further. When changing code to print messages to the screen, the best approach is to add the messages in a way that can be easily enabled or disabled depending on whether we want the debug info or not. In Python, we can do this using the logging module. This module, lets us set how comprehensive we want our code to be. We can say whether we want to include all debug messages, or only info warning or error messages. Then when printing the message, we specify what type of message we're printing. That way, we can change the debug level with a flag or configuration setting. So you figured out why the unexpected exception was thrown, what do you do next? The solution might be fixing the programming error like making sure variables are initialized before they're used or that the code doesn't try to access elements after the end of a list. Or it could be that certain use cases that hadn't been considered needs to be added to the code. In general, you'll want to make the program more resilient to failures. Instead of crashing unexpectedly, you want the program to inform the user of the problem and tell them what they need to do. For example, say you have an application that crashes with a permission denied error. Rather than the program finishing unexpectedly, you'll want to modify the code to catch that error and tell the user what the permission problem is so they can fix it. For example, unable to write new files and temp, make sure your user has bright permissions on temp. In some cases, it doesn't make sense for our program to even run if certain conditions aren't met. In that case, it's okay for the program to finish when the error is triggered. But again, it should do so in a way that tells the user what to do to fix the problem. For example, if it's critical for an application to connect to a database but the database server isn't responding, it makes sense for the application to finish with an error saying unable to connect to the database server. It also makes sense to include all details of the attempted connection like the host name, the port, or the username used to connect. So to recap, if your program is crashing with an unhandled error, you want to first do some debugging to figure out what's causing the issue. Once you figured it out, you want to make sure that you fix any programming errors and that you catch any conditions that may trigger an error. This way, you can make sure the program doesn't crash and leave your users frustrated. Up next, we'll talk a bit about what you can do when you're trying to fix someone else's code.

fixing someone else's code
In our IT jobs, it's pretty common to have to fix problems and code that we didn't write ourselves. It might be because we're working with a program that's open-source or with a program that was developed by someone else inside the company. When this happens, we need to spend some time getting acquainted with the code so that we can understand what's going on. Let's do a rundown of some things that can help us with that. If the code has comments and the functions are well-documented, reading these is a great place to start when trying to figure out what's going on. Remember way back in the course when we first introduced Python, we talked about the importance of developing good habits when we're writing code. Writing good comments is one of those good habits that pays off when trying to understand code written by others and also your past self. Unfortunately, a lot of code doesn't include enough comments, leaving us to try to understand it without enough context. If that's the case, you can improve things by adding comments as you read the code and figure out what it's doing. Writing these comments help you solidify your understanding. If you contribute those comments back to the original developers, you can help anybody else trying to understand the code. Another thing that can help to understand someone else's code is reading the tests associated to the code. Well-written tests can tell us what each function is expected to do. Looking at the existing tests can show us which use cases weren't taken into account. But what if there aren't enough tests? Just like with writing extra comments, writing some tests of your own can help you better see what the code is supposed to do and improve overall quality of the code. This can also be really useful when modifying the original code. To ensure that changes you make, don't break the rest of the functionality. In my job, I need to make changes to code written by other people a lot. I definitely read the comments and sometimes reference the tests too. But in the end, to really understand what's going on, I just have to read through the code. But how do you even start reading through someone else's code? This depends a bit on personal preference and the size of the project. If there are only a couple of 100 lines of code, it's feasible to read all of them. But when the project has thousands or tens of thousands of lines of code, you can't really read the whole thing. You'll need to focus on the functions or modules that are part of the problem that you're trying to fix. One possible approach in this case, would be to start with the function where the error happened, then the function or functions that call it, and so on until you can grasp the contexts that led to the problem. While this is of course much easier if it's in a programming language that you're familiar with, you don't need to be an expert in the language to fix a bug in the program. If you've come across an error and debug the issue well enough to understand what's going on, you might be able to fix the problem even if you've never seen that language before. This is one of those skills that gets better with practice. So it might make sense to you to start practicing before you need to fix a problem in the code. Take a program that you both use and have access to its code and figure out how it does a specific action. Follow the code until you really understand what's going on. For example, you could take the web server software you're using and check out how it parses its configuration files, or take a look at one Python module you like, like Python Request for example, and figure out how it processes the data it receives. Doing this, you can get used to reading code written by others and understanding what it's doing. Another option is to pick an open-source project that you use. Look at the list of open issues and to have a go at fixing an easy one. To do that, you'll need to find your way around the code, understand what it's doing and what to change. By practicing doing this, you'll improve your ability to quickly figure out what the code does and what needs to be changed, while helping improve the project's overall quality. Up next, we'll get some practice fixing issues and a couple of different programs that crash.

crashes in complex s/m
Up to now we've talked about how to diagnose and fix errors that are confined to one computer. That's a common case for computers that are used by a single user. But once we start going into complex systems that involve many different services, we'll need to take a look at the bigger picture and have different computers interact with each other. Say you're in charge of the e-commerce site for your company. The page as seen by the users recently started responding with internal server error to about 20% of all requests. How do you figure out what's going on? You want to apply the same principles that we saw for troubleshooting a problem on one computer, but this time at a larger scale. So you'll want to check the log messages in the servers providing the service, and see if you find any additional information pointing to what's causing the issue. You'll want to find any log specific to the service that's failing, and also look at the general system logs to see if there's a problem affecting the server in general. For this example, let's say you find a bunch of entries in the logs that say, invalid response from server. That's not a great error message. You don't know what the request was or what the response was, but it's at least a clue that whatever's happening is related to some other service in the overall system. We said that this started failing recently, so it might make sense to figure out what changed between what it was working correctly and when it started to fail. Was there a new version of the system deployed? Were there any relevant changes regarding the requests? Let's say this is happening on a Tuesday morning, and the latest release of this service was the previous week. Things were working fine until today, and the requests seemed normal, nothing out of the ordinary. So the service itself is probably okay, but what about the other services involved in the system? Was there a new version of one of the underlying systems, like the database, the authentication service, or some other back-end server like the inventory, billing, or procurement systems? Looking at recent changes, you see that there were a bunch of changes made earlier in the day to the load balancer used between the front-end and the back-end services. Since the only clue you have is that the response from the service was invalid, you're not sure that these changes are at fault, but they sure seem suspicious. Whenever possible, the best strategy is to roll back the changes that you suspect are causing the issue, even if you aren't 100% sure if this is the actual cause. If your infrastructure allows easy rollbacks, try that before doing any further investigation. Why? Because that way, you'll restore the service back to health if it was the cause, or you'll eliminate this change as a possible cause if doing the rollback doesn't help. Whether you do the rollback or not, when coming across unhelpful error messages, it's a good idea to improve them. Instead of the error just saying that the response is invalid, change it to include what the request and the response were, and why the response was invalid. That way, the next time you're trying to debug a similar issue you already have more information to work with. For this example, if the error had included this information you'd have seen that the invalid response was a 404 error. This was caused by having a server added to the pool as part of the inventory system, but the server actually belonged to the procurement system. Now, say a couple of weeks later you see that again, there are a bunch of internal server errors in the same service. It might be tempting to assume that it's the load balancer's fault once again, but by now you know that you should always look at the logs first and see what you find. There's no reason why the error should be the same this time. When looking at the logs you may notice, for example, that only one of the front-end servers is actually affected by the problem. All the other machines are serving their content successfully. In a case like this, you'd start by first removing the machine from the pool of servers that can provide this service. That way, you avoid users getting any more errors. Well, you can investigate what's going on with the broken machine. As you've probably realized by now, when dealing with complex systems like these having good logs is essential to understanding what's going on. On top of that, you'll want to have good monitoring of what the service is doing and use version control for all changes so that you can quickly check what's changed and roll back when needed. It's also important that you can very quickly deploy new machines when necessary. This could be achieved by either keeping standby servers, in case you need to use them, or by having a tested pipeline that allows you to deploy new servers on demand.
Start transcript at 4 minutes 55 seconds4:55
A lot of companies today have automated processes for deploying services to virtual machines running in the cloud. This can take a bit of time to set up, but once you've done that you can very easily increase or reduce the amount of servers you're using. This can help a lot when investigating and solving problems. But one thing to take into account when the servers are running as virtual machines, especially if they're running in the cloud, is that there might be external limits apply to these services. Resources, like the available CPU time, RAM, or network bandwidth, might be artificially capped. And not only that, the use of certain external services can also be limited, like how many database connections you can have at the same time or how much data you can store. If these limits are causing problems with your application, you might need to rethink how you use your resources.
Start transcript at 5 minutes 50 seconds5:50
We've covered a bunch of techniques that you can use when facing a problem in a complex system. Looking at the available logs, figuring out what changed since the system was last working, rolling back to a previous state, removing faulty servers from the pool, or deploying new servers on demand. Up next, we'll explore a different part of dealing with bigger incidents, communication and documentation.

Communication and Documentation During Incidents
Until now, we've discussed how we can troubleshoot computers or systems with a specific issue. We've covered how we can get enough information so we can identify the root cause, and then apply the necessary remediation. There's another aspect to all of this. What is related to how we handle the communication with those affected by the issue and how we distribute tasks when addressing large issues as a team. Armed with what you've learned so far and your past experience, you might do a great job troubleshooting a problem. But if you drop the ball when it comes to communicating what you're doing, you could end up with a bunch of frustrated users calling you to find out what's going on. If you don't write down what you've tried or how you fix the problem, you risk for getting some important details and wasting a lot of valuable time when you need to revisit an issue. When working on a problem, it's always a good idea to document what you're doing in a bug or ticket. If there's no such system at your company, then use a doc, a text file, or Wiki, or whatever you have access to. Documenting what you do, lets you keep track of what you've tried and what the results were. This might seem unnecessary. But after a whole day of troubleshooting a problem, it's pretty common for us to forget what we've tried or what was the outcome of a specific action. On top of that, having all this info available in some electronic forum lets you easily share all the data you've collected with other team members. If for example, you brought something back which turned out to be unrelated. Having the whole process document it, helps you remember to roll forward again. While you're working on a problem, it's important to communicate clearly with those affected by the issue. They want to know what you figured out about the problem, what the available workarounds are, and when they can expect the next update. If you don't know what the problem is, it's hard to give an estimation of when you'll have it fixed. But you can still provide timely updates about the work you're doing. This kind of regular communication is helpful no matter the size of the incident. But the more people affected, the more you'll want to provide regular updates with clear instructions of what users can do and what they can expect as a solution. That way, they can better plan and organize their time. If access to the Internet is down, you want to let people know if they can expect to fix in one or two hours or if it's going to take the whole day. This info can make a difference between people choosing to discuss issues in person for a couple of hours or deciding to work from home. If the issue is big enough that you're involving more people in finding a solution, you should agree on who's going to work on which tasks. For example, you could have someone working on finding out a temporary workaround, while someone else is in charge of understanding the root cause of the problem and finding the long-term remediation. Or if there are lots of possible causes for the issue, you could divide the causes among the team members and have them work on those in parallel. On top of people looking for the root cause and a solution, you want to have a person in charge of communicating with the people affected. This lets the team avoid forgetting to update the tracking issue or even worse providing contradictory information. This communications lead needs to know what's going on and provide timely updates on the current state and how long until the problem's resolved. They can act as a shield for questions from users letting the rest of the team focus on the actual problem. Similarly, there should be one person in charge of delegating the different tasks to the team members. This person sometimes called the Incident Commander or Incident Controller needs to look at the big picture and decide what's the best use of the available resources. They can make sure that there's no duplication of work among team members and that only one person is modifying the production system at a time. Having multiple people make overlapping changes to the system could lead to confusing results, making the outage even longer. Of course, this division of roles makes the most sense when there's a large incident and there's a big team working on figuring out the solution. If it's only two or three people working on the problem, it's still important to agree who will work on what but you probably don't need to use any special role names to do that. Once the issue has been resolved, it's super-important to sum up the information that was helpful. The most important information that you'll want to include are the root cause, how you diagnose the problem and found that root cause, what you did to fix the issue and what needs to be done to prevent the problem from happening again. Depending on the size of the issue and the number of people affected, this summary could just be the last update to the bug or ticket that you use to keep track of your work, or it could be a full postmortem. What's a postmortem, and how do you write when you ask? Well, that's coming up in our next video.

writing post mortem
In our last video, we talked about the importance of communication and documentation when troubleshooting incidence. We called out that if the issue is big enough, we might want to document what happened in a postmortem. Postmortems are documents that describe details of incidence to help us learn from our mistakes. When writing a postmortem, the goal isn't to blame whoever caused the incident, but to learn from what happened to prevent the same issue from happening again. To do this, we usually document what happened, why it happened, how it was diagnosed, how it was fixed, and finally figure out what we can do to avoid the same event happening in the future. Remember the main goal is to learn from our mistakes. Writing a postmortem isn't about getting someone fired but about making sure that next time we do better. Writing postmortems after dealing with incidence is important because it helps us avoid dealing with them again or at least learn how to deal with the next incident better. While Postmortems are super useful with large incidence, you don't need to wait until something huge happens to write your first postmortem. You can practice riding them for any kind of event where there's something to be learned no matter how small. That way, when you need to write a postmortem after a big incident, you know how to concentrate on the things that matter the most. What you can learn from the problem and how you can prevent it in the future. So what should you write in a postmortem? The exact structure might vary depending on preference and the type of incident that you're dealing with. In general, you'll want to include the details of what caused the issue, what the impact of the issue was, how it got diagnosed, the short-term remediation you applied, and the long-term remediation you recommend. If the document is long and you're going to share it with a lot of people, you want to include a summary that highlights the root cause, the impact, and what needs to be done to prevent the issue from happening again. It's useful to include what went well in postmortems too. When working on a problem, we might realize that it would have been much worse if we didn't have certain tools or systems available. For example, we might say that we were able to solve the problem quickly by doing a roll back to the previous version or that we caught the issue before users even noticed it because we had good monitoring and alerting. Noting the things that went well helps us show that our systems are effective and justifies keeping those systems running. Writing a postmortem can sometimes help you understand the services that you're working with much better. Earlier this year, a service I worked on had a large outage and I needed to provide information on what happened. To do this, I needed to parse through hundreds of gigabytes of archive logged data to show that certain data had never been received by the service. Doing this, I realized that I needed to improve the data logged by our tools to give better information and have better reporting. You can even practice writing postmortems outside of the IT context. Like, if you bake cookies and they don't turn out as great as you wanted them to, document what you did, what went wrong, what went right, and how you can improve the results in the future. You can do this with any hobby that you have. Maybe photography, 3D printing or brewing your own beer. You don't always need to write the whole thing down. Sometimes a mental note is enough, like if you bike to work and realize wearing your backpack hurts your shoulders, make a mental note to add a basket to your bike. So you can put your backpack there next time, or if on your last trip it was colder than expected and you forgot to bring a jacket, make a mental note that next time you should check the weather before you leave. Once again, remember that the most important part of the postmortem is what we can learn for the future. So if instead of writing a whole document you're creating a one paragraph summary of the incident. Remember to focus that paragraph on what you can do better, not on whatever mistake caused the incident. Up next, we've got a quick quiz to check that everything still make sense.
